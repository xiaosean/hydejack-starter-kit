---
# Posts need to have the `post` layout
# layout: post
# comments: true

# The title of your post
# title: GLNet簡介 - GLNet for Memory-Efficient Segmentation of Ultra-High Resolution Images


# (Optional) Write a short (~150 characters) description of each blog post.
# This description is used to preview the page on search engines, social media, etc.
description: >
  Wuyang Chen, Ziyu Jiang, Zhangyang Wang, Kexin Cui, Xiaoning Qian. [GLNet for Memory-Efficient Segmentation of Ultra-High Resolution Images](https://arxiv.org/abs/1905.06368). In CVPR'19.
 
# (Optional) Link to an image that represents your blog post.
# The aspect ratio should be ~16:9.
<!-- image: /assets/img/default.jpg -->
<!-- hide_image: true -->

# You can hide the description and/or image from the output
# (only visible to search engines) by setting:
# hide_description: true
# hide_image: true

# (Optional) Each post can have zero or more categories, and zero or more tags.
# The difference is that categories will be part of the URL, while tags will not.
# E.g. the URL of this post is <site.baseurl>/hydejack/2017/11/23/example-content/
categories: [Deep Learning, Computer Vision]
tags: []
# If you want a category or tag to have its own page,
# check out `_featured_categories` and `_featured_tags` respectively.
---
CVPR 2019 paper

Paper link: https://arxiv.org/abs/1905.06368

Github(Pytorch): https://github.com/TAMU-VITA/GLNet

Oral presentation: [CVPR'19 Oral Memory-Efficient Segmentation of Ultra-High Resolution Images]

# 簡介

近年來圖像分割的任務開始著重在高解析度(Ultra-high resolution)圖片，

![](/assets/img/2020-01-07-GLNet/fig2.png)

而礙於高解析度的圖像需要考量 GPU 的記憶體，

近年來的方法是透過縮小圖像(Downsample)或是圖像切成小塊(Crop)去做預測，

而這些方法都會降低一點準確度，

分別是喪失細節特徵和喪失整體內容特徵 (Context)。

![](/assets/img/2020-01-07-GLNet/fig3.png)

本文提出不只要注重細節也要考量整體的特徵，

基於此想法提出 GLNet - Collaborative Global Local Networks

其模型的主要想法為
- 保留細節特徵
- 保留整體的內容特徵
- 節省記憶體（因高解析度的圖片需要大量的記憶體)

GLNet 是使用縮小的圖片獲得整體的內容特徵(Global)，

接著使用 Patch 的概念，

將圖片分割成小圖片藉此獲得細節特徵(Local)，

透過兩者互相搭配可獲得更為精確的結果。

而 GLNet 節省記憶體方式是透過將小圖片拼湊成大圖片，

藉此我們可以不用輸入原先高解析度的圖片，

而是使用 Downsample 的圖片以及 Crop 圖片中的不同區域，

因此輸入圖片的 Size(實驗中設定為 508 x 508) 並不大。

透過這技巧 GLNet 可適用於高解析度的圖片(最高 30M pixels, 解析度約為 6749 X 4499)，

但卻不會造成記憶體不足的問題。

![](/assets/img/2020-01-07-GLNet/table1.png)

因此 GLNet 只需一張 1080 Ti 即可 Training，

而 Inference 只需不到 2 GB 的記憶體。

![](/assets/img/2020-01-07-GLNet/fig1.png)

從上圖可以看到 GLNet 使用的記憶體是相當少的但卻能達到非常不錯的準確度。

# 方法

![](/assets/img/2020-01-07-GLNet/oral-presentation.png)

上圖出自 [CVPR'19 Oral Memory-Efficient Segmentation of Ultra-High Resolution Images]

模型架構分為 Global branch 以及 Local branch，

分別負責圖片整體以及圖片的細節，

Global branch （G）會將圖片縮小為 Size1(實驗設定 508 x 508) 後輸入(代稱 Downsample 圖片)，提取出整體的內容。

Local branch （L）會將圖片劃分為不同區塊，每一塊的大小也為 Size1(代稱 Crop 圖片)，用於萃取出圖片的細節部分。

最終將 G 和 L 所得出的的 Feature map 做整合(Aggregation)當作最後的輸出。

透過這 2 個 Branch 的互相搭配，

我們可以獲得一個精準的 Crop 圖片的圖像分割結果，

透過這方式不斷的 Inference 每個 Crop 的圖片來獲得整張圖像的圖像分割結果。
 
細節部分：
1. Global branch 和 Local branch 各自由一個 Resnet 50 所組成。
2. G 和 L 在整合(Aggregation)時，G 會對輸出的結果進行裁切，對應各個 Patch 的位置，再縮放成 L 的大小。
此時這兩個 Feature maps雖都是從同樣位置提取出來的，但代表的含義不同， G 是基於整張圖片的理解。而 L 則是只看圖片中的某一區塊所得來的 Feature maps。
> ![](/assets/img/2020-01-07-GLNet/fig5.png)
> 這邊是將兩個 Branch 進行合作


3. 基於上方的說明，在 Aggregation 前兩個 Feature maps 應該要相似，都是看圖片當中的同一個位置，所以可以給一個 Loss，希望兩者所得出來的輸出相似，這邊使用 MSE loss。此處稱作 Branch Aggregation with Regularization。





 

# 參考資料：

[Regularizing Deep Networks by Modeling and Predicting Label Structure]

[Regularizing Deep Networks by Modeling and Predicting Label Structure]:https://arxiv.org/abs/1804.02009

[CVPR'19 Oral Memory-Efficient Segmentation of Ultra-High Resolution Images]:https://www.youtube.com/watch?v=am1GiItQI88